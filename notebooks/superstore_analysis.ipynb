{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e90768c",
   "metadata": {},
   "source": [
    "# Superstore Sales Dashboard — Data Analysis\n",
    "**Author:** Nick Trybushkin  \n",
    "**Goal:** Perform exploratory data analysis and build a Power BI dashboard using Superstore dataset.  \n",
    "**Tools:** Python (Pandas, NumPy, Matplotlib, Seaborn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../data/Sample - Superstore.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# df.head()\n",
    "# df.shape\n",
    "# df.info()\n",
    "\n",
    "df.columns = [col.strip().replace(\" \",\"_\").lower() for col in df.columns]\n",
    "# df.sample(100).to_csv(\"../data/superstore_sample.csv\", index=False)\n",
    "\n",
    "# C. Identify numeric and non-numeric fields\n",
    "\n",
    "# df.select_dtypes(include=\"number\").columns\n",
    "# df.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "# D. Check for missing or null values\n",
    "# df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# E. Basic statistics summary\n",
    "df.describe()\n",
    "\n",
    "# F. Inspect column names manually\n",
    "# df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f2cbc",
   "metadata": {},
   "source": [
    "### Initial observations\n",
    "\n",
    "- Dataset has 9994 rows with 21 columns.\n",
    "- No critical missing values found.\n",
    "- 'order_date' and 'ship_date' will need to be converted to datetime.\n",
    "- Sales and profit are highly skewed distributions with several extreme outliers - particulary large Sales values and both high and low Profit values.\n",
    "- Discount ranges from 0 - 0.8, which may influance profitability.\n",
    "- Region, Category, and Sub-category are categorical and suit grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17402a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Basic Statistics Check\n",
    "# Goal:\n",
    "\n",
    "# Use descriptive statistics and simple Pandas checks to detect numerical issues such as outliers, extreme discounts, negative profits, or inconsistent values.\n",
    "\n",
    "# Generate Descriptive Statistics\n",
    "df.describe().transpose()\n",
    "\n",
    "# Inspect key numeric columns individually\n",
    "\n",
    "df['sales'].describe()\n",
    "# has extremely high max value: 22638.480000\n",
    "# because of that mean is high: 229 and std: 623 is larger than mean\n",
    "\n",
    "df['profit'].describe()\n",
    "# extremely min: -6599 and max: 8399\n",
    "# std: 234 is larger than mean: 28\n",
    "\n",
    "df['discount'].unique()[:10]\n",
    "# no values > 1\n",
    "\n",
    "df['discount'].describe()\n",
    "# std: 0.2 > mean: 0.15\n",
    "\n",
    "df['quantity'].describe()\n",
    "# no 0 or extra large values - no issues\n",
    "\n",
    "# C. Detect outliers with simple conditions\n",
    "df[df['discount'] > 0.8]\n",
    "# usually discout shouldn't be higher than 0.8, if there are such , need follow-up\n",
    "# no such rows\n",
    "\n",
    "df[df['profit'] < -500]\n",
    "# 50 rows, will keep them\n",
    "# will heavily influence visualization\n",
    "\n",
    "df[df['quantity'] % 1 != 0]\n",
    "# shoudln't be any decimal numbers\n",
    "\n",
    "df[df['sales'] > 5000]\n",
    "# 19 such rows\n",
    "# rare but legitimate\n",
    "\n",
    "# Check numeric relationships\n",
    "\n",
    "df[df['profit'] > df['sales']]\n",
    "# 0 rows\n",
    "# profit shouldn't be more than sales\n",
    "# if not, investigate\n",
    "\n",
    "\n",
    "df[['quantity', 'sales']].corr()\n",
    "# expect to see the moderate positive correlation, here: 0.2\n",
    "# if no, require investigation\n",
    "\n",
    "# Profit should not be 0 when sales > 0\n",
    "df[(df['sales'] > 0) & (df['profit'] == 0)]\n",
    "# Found 65 rows, should be rare\n",
    "\n",
    "# Discount should reduce profit\n",
    "df[(df['discount'] > 0.5) & (df['profit'] > 0)]\n",
    "# found 0 rows, as high discound and profit are suspicious\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd7efb",
   "metadata": {},
   "source": [
    "### Basic Statistic Observation\n",
    "\n",
    "- Sales range from 0.44 to 22638, because of that mean is high: 229 and std: 623 is larger than mean.\n",
    "- Profit range from -6599 to 8399, negative values occur because of the large discount, std: 234 is much larger than mean: 28.\n",
    "- Discounts vary from 0 to 0.8, no values above 0.8 detected, std: 0.2 > mean: 0.15.\n",
    "- Quantity has range from 1 to 14; all values appear reasonable.\n",
    "- No inconsinstancies found (e.g. profit > sales).\n",
    "- Outliers will be kept for now, but need special attention during visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Structure Understanding\n",
    "\n",
    "# List all categorical columns\n",
    "categorical_cols = df.select_dtypes(exclude='number').columns\n",
    "df[categorical_cols].nunique().sort_values(ascending=False)\n",
    "\n",
    "# Very high-cardinality columns (like product_name, customer_name) → useful for top/bottom analysis.\n",
    "# Medium-cardinality columns: city, state\n",
    "# Low-cardinality columns: region, category, segment, ship_mode → great for grouping.\n",
    "\n",
    "# Inspect key categorical fields\n",
    "\n",
    "# Category\n",
    "df['category'].value_counts()\n",
    "# expect: Office Supplies > Furniture > Technology, it's correct\n",
    "\n",
    "# sub-category\n",
    "df['sub-category'].value_counts().head()\n",
    "# shows detailed product types\n",
    "\n",
    "# region\n",
    "df['region'].value_counts()\n",
    "\n",
    "# ship_mode\n",
    "df['ship_mode'].value_counts()\n",
    "\n",
    "# customer segment\n",
    "df['segment'].value_counts()\n",
    "\n",
    "# These are dimensions i will analyze mostly\n",
    "\n",
    "# Look at relationships between key categories\n",
    "df.groupby('category')['sub-category'].nunique()\n",
    "\n",
    "df.groupby('region')['state'].nunique()\n",
    "\n",
    "# Quick peek\n",
    "df.groupby('category')['profit'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Identify high-level dimensions\n",
    "# Main buisness dimension to analyze\n",
    "\n",
    "# Time -> order_date, ship_date\n",
    "# Location -> region, state, city\n",
    "# Customer -> segment, customer_name\n",
    "# Product -> category, sub-category, product_name\n",
    "# Shipping -> ship_mode\n",
    "# Metrics -> sales, profit, discount, quantity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24931b9",
   "metadata": {},
   "source": [
    "### Structure Understanding — Key Insights\n",
    "\n",
    "- Dataset contains several important categorical dimensions:\n",
    "  - **Products:** Category (3), Sub-category (17), Product Name (~1850)\n",
    "  - **Geography:** Region (4), State (~49), City (~500)\n",
    "  - **Customers:** Segment (3), Customer Name (~800)\n",
    "  - **Shipping:** Ship Mode (4)\n",
    "- Product and geography dimensions will be crucial for KPI analysis.\n",
    "- High-cardinality fields (product_name, customer_name) are useful for ranking (top/bottom lists).\n",
    "- Category → Sub-category relationship is well-defined and consistent.\n",
    "- Region → State → City hierarchy appears logical and complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6111af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Data Cleaning\n",
    "# Goal:\n",
    "\n",
    "# Fix data types, remove duplicates, standardize values, create new useful columns.\n",
    "# This prepare the dataset for analysis and PowerBI dashboarding\n",
    "\n",
    "# A. Convert Date columns to Datetime\n",
    "\n",
    "# 1. Identify date columns\n",
    "# order_date, ship_date\n",
    "\n",
    "# 2. Convert\n",
    "df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "df['ship_date'] = pd.to_datetime(df['ship_date'])\n",
    "\n",
    "# 3. Verify\n",
    "df[['order_date', 'ship_date']].dtypes\n",
    "\n",
    "# 4. Check for conersion errors\n",
    "df[df['order_date'].isna()]\n",
    "df[df['ship_date'].isna()]\n",
    "\n",
    "# 5. Optional, extract components (will be used later)\n",
    "df['order_year'] = df['order_date'].dt.year\n",
    "df['order_month'] = df['order_date'].dt.month\n",
    "df['order_day'] = df['order_date'].dt.day\n",
    "\n",
    "df.head()\n",
    "\n",
    "# B. Sort Dataset by Order Date\n",
    "# Chronologically organize the data so all time-based analysis (trends, growth, seasonality) is correct and intuitive\n",
    "\n",
    "# 1. Sort values\n",
    "df = df.sort_values('order_date')\n",
    "\n",
    "# 2. Reset index\n",
    "# drop=True ensures Pandas won't add the old index as column\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 3. Verify sorting work\n",
    "# print(df[['order_date', 'ship_date']].head())\n",
    "# print(df[['order_date', 'ship_date']].tail())\n",
    "\n",
    "# 4. Optionally, sort both order and ship dates\n",
    "df = df.sort_values(['order_date', 'ship_date'])\n",
    "\n",
    "# 5. Validate chronological correctness\n",
    "# shipping should always occur after order\n",
    "df[df['ship_date'] < df['order_date']]\n",
    "# should be empty\n",
    "\n",
    "# Summary\n",
    "# We have\n",
    "# Proper datetime columns\n",
    "# Chronologically ordered data\n",
    "# Clean index\n",
    "# Validated date structure \n",
    "\n",
    "# C. Check and remove duplicates\n",
    "# Identify and remove duplicate rows to avoid inflating metrics like total sales, total profit, etc.\n",
    "\n",
    "# 1. Check duplicates\n",
    "df.duplicated().sum()\n",
    "\n",
    "# 2. Preview duplicate rows (if any)\n",
    "df[df.duplicated()].head()\n",
    "\n",
    "# 3. Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "# print(df.head())\n",
    "\n",
    "# 4. Check for duplicated order IDs\n",
    "# order id can repeat , as single order can have multiple products\n",
    "\n",
    "df['order_id'].nunique(), df.shape[0]\n",
    "# Interpretation:\n",
    "# fewer unique order IDs than rows → correct\n",
    "# many items per order expected\n",
    "\n",
    "# Summary, you only remove the rows where every column identical\n",
    "\n",
    "# D. Understand if any column has missing values and decide what to do with them.\n",
    "\n",
    "# 1. Count missing values\n",
    "df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# 2. Visual scan of missing data, sometimes useful\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.heatmap(df.isna(), cbar=False)\n",
    "# plt.show()\n",
    "\n",
    "# This gives a matrix-like visualization.\n",
    "# If you see bright vertical lines → many missing values.\n",
    "# Normally, Superstore dataset will look mostly solid (few or no gaps).\n",
    "\n",
    "\n",
    "# If missing values exist - choose strategy\n",
    "\n",
    "# 1. For categorical column\n",
    "\n",
    "# Examples: region, segment, ship_mode\n",
    "\n",
    "# Options:\n",
    "\n",
    "# Fill with most frequent value\n",
    "# df['column'] = df['column'].fillna(df['column'].mode()[0])\n",
    "\n",
    "# Fill with unknown\n",
    "# df['column'] = df['column'].fillna('unknown')\n",
    "\n",
    "# For numeric columns\n",
    "# exmaple: sales, profit, discount\n",
    "\n",
    "# options\n",
    "\n",
    "# Fill with 0\n",
    "# df['column'] = df['column'].fillna(0)\n",
    "\n",
    "# Fill with median\n",
    "# df['column'] = df['column'].fillna(df['column'].median())\n",
    "\n",
    "# For Date columns\n",
    "# Drop only if all missed\n",
    "\n",
    "# df = df.dropna(subset=['order_date','ship_date'])\n",
    "\n",
    "# Verify no missing values remain\n",
    "df.isna().sum().sum()\n",
    "\n",
    "# Check for empty strings\n",
    "(df == \"\").sum()\n",
    "# If any column shows count > 0, treat it like missing values \n",
    "\n",
    "# Summary of Step D\n",
    "\n",
    "# By the end of this step, you will:\n",
    "# Know exactly which columns had missing values\n",
    "# Apply appropriate imputation if needed\n",
    "# Confirm the cleaned dataset has no NaNs or empty fields that break analysis\n",
    "\n",
    "# E. Validate Numeric Consistency\n",
    "\n",
    "# Goal: check that numeric fields follow business rules\n",
    "# This protects your analysis from bad data (e.g. negative quantities, impossible discounts)\n",
    "\n",
    "# 1. Check for invalid quintity value\n",
    "\n",
    "# Quantity should always be 1 or more\n",
    "df[df['quantity'] < 1]\n",
    "\n",
    "# Fix if needed\n",
    "df = df[df['quantity'] > 0]\n",
    "\n",
    "# 2. Validate discount values\n",
    "# Discount must be > 0 and < 1\n",
    "df[(df['discount'] < 0) | (df['discount'] > 1)]\n",
    "\n",
    "# Fix if needed\n",
    "df.loc[df['discount'] > 1, 'discount'] = 1\n",
    "df.loc[df['discount'] < 0, 'discount'] = 0\n",
    "\n",
    "# Check that profit is not greater than sales\n",
    "df[df['profit'] > df['sales']]\n",
    "\n",
    "# Should be empty, if not, can be due to incorrect discount\n",
    "# Example investigation\n",
    "df[df['profit'] > df['sales']].head()\n",
    "# than decide what to do (drop or adjust)\n",
    "\n",
    "# 3. Look for suspicious negative sales\n",
    "# sales should never be negative\n",
    "df[df['sales'] < 0]\n",
    "\n",
    "# Fix if neede\n",
    "df = df[df['sales'] >= 0]\n",
    "\n",
    "# profit can be, because of high discount\n",
    "# sales - never\n",
    "\n",
    "# 4. Check for zero or near-zero sales\n",
    "df[df['sales'] < 1].head()\n",
    "\n",
    "# sometimes tiny values are normal\n",
    "# but if it's 0.00 may indicate incorrect entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f191442",
   "metadata": {},
   "source": [
    "### Numeric consistency check\n",
    "\n",
    "- No negative quantities found.\n",
    "- Discounts range is from 0 to 0.8.\n",
    "- No cases when profit more than sales.\n",
    "- No negative sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a795c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F. Detect outliers without removing them\n",
    "\n",
    "# Identify unusual alues that can distort graphs or analysis\n",
    "# We do NOT remove outliers, they are often real business cases, especially discounts\n",
    "\n",
    "# 1. Detect high discounts\n",
    "df[df['discount'] > 0.5].head()\n",
    "\n",
    "# Deep discounts often produce negative profits\n",
    "\n",
    "# 2. Detect extremely negative profits\n",
    "# Profit below -2000 is rare and usually indicates the large discount item\n",
    "\n",
    "df[df['profit'] < -2000].head()\n",
    "\n",
    "# These values are valid but important for\n",
    "# Boxplot\n",
    "# Distibutions\n",
    "# Correlation analysis\n",
    "# You will later mention them as outliers later\n",
    "\n",
    "# 3. Detect unusual high sales values\n",
    "df[df['sales'] > 5000].head()\n",
    "# These are usually legitimate large furniture purchases\n",
    "\n",
    "# 4. Look at distribution shapes\n",
    "\n",
    "# Sales distribution\n",
    "# df['sales'].plot(kind='hist', bins=50, figsize=(7,4))\n",
    "\n",
    "# Profit distribution\n",
    "# df['profit'].plot(kind='hist', bins=50, figsize=(7,4))\n",
    "\n",
    "# You will notice: \n",
    "# Profit has many negative values\n",
    "# Sales is right skewed, typically in retail\n",
    "\n",
    "# 5. Optional: use IQR method to flag outliers, NOT DELETE\n",
    "q1 = df['sales'].quantile(0.25)\n",
    "q3 = df['sales'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "outliers = df[(df['sales'] < q1-iqr*1.5) | (df['sales'] > q3+iqr*1.5)]\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa3cc9",
   "metadata": {},
   "source": [
    "### Outliers analysis\n",
    "\n",
    "- High discounts (> 50%) found primarily in Furniture category.\n",
    "- Several extremely negative profits (<-2000) observed - all result of high discounts.\n",
    "- Large sales values (> 5000) represent big furniture orders.\n",
    "- Profit distribution is heavily tailed with many negative cases.\n",
    "- Outliers will be kept for analysis, but noted in reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28c24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G. Create new calculated columns\n",
    "# Add derived variables that enhance your analysis, and help answer business questions.\n",
    "# These fields will be used again and again in grouping, filtering and visualizing results\n",
    "\n",
    "# 1. Profit Margin\n",
    "# Shows how much of each dollar of each sale is profit\n",
    "\n",
    "df['profit_margin'] = df['profit'] / df['sales']\n",
    "# Can be negative (loss)\n",
    "# Can't be > 1 (if so, investigate later)\n",
    "\n",
    "df['profit_margin'].describe()\n",
    "\n",
    "# Key insights:\n",
    "\n",
    "# The mean profit margin is 12%, which indicates that the business operates with relatively low overall margins.\n",
    "# The median margin is significantly higher (27%), suggesting that most transactions are profitable and fall into the 20–30% range.\n",
    "# The minimum margin is –275%, indicating the presence of extreme loss-making orders, likely caused by heavy discounts, returns, or data anomalies.\n",
    "# The maximum margin is 50%, which is realistic for certain high-markup items.\n",
    "# The standard deviation is very high (0.466), reflecting large variability in profitability across products and categories.\n",
    "# The data includes notable negative outliers, which pull the mean down and should be investigated separately.\n",
    "\n",
    "# Business Implication:\n",
    "\n",
    "# The company is mostly profitable, but a small number of orders — likely related to discounts, logistics costs, or data \n",
    "# quality issues — generate disproportionately large losses. These outliers need further investigation to determine whether they represent valid business scenarios or data inconsistencies.\n",
    "\n",
    "# 2. Order year\n",
    "df['order_year'] = df['order_date'].dt.year\n",
    "# Grouping by year is essential for YOY (year-over-year) analysis\n",
    "\n",
    "df['order_year'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# 3. Order month\n",
    "# For monthly trend analysis\n",
    "df['order_month'] = df['order_date'].dt.month\n",
    "df['order_month'].value_counts().sort_values()\n",
    "\n",
    "# 4. Month-year combination\n",
    "df['order_month_year'] = df['order_date'].dt.to_period('M').astype(str)\n",
    "df['order_month_year'].value_counts().sort_values()\n",
    "# Very useful for continues trend plots (e.g. line chart)\n",
    "\n",
    "# 5. Shipping delay (days)\n",
    "# Difference between ship_date and order_date indicates delivery speed\n",
    "df['shipping_delay'] = (df['ship_date'] - df['order_date']).dt.days\n",
    "df['shipping_delay'].describe()\n",
    "\n",
    "# Flag negative delays, should be none\n",
    "df[df['shipping_delay'] < 0]\n",
    "\n",
    "# 6. Revenue buckets (categorical-bins)\n",
    "# Useful for segmentation in dashboards\n",
    "df['sales_bucket'] = pd.cut(df['sales'], bins=[0,100,500,1000,5000,20000], labels=['0-100','100-500','500-1000','1000-5000','5000+'])\n",
    "df['sales_bucket'].value_counts().sort_values()\n",
    "\n",
    "# H. Verify calculated columns\n",
    "# Insure new created columns correct\n",
    "\n",
    "# 1. Preview all new columns\n",
    "df[['sales', 'profit', 'profit_margin', 'order_year', 'order_month', 'order_month_year', 'shipping_delay']].head()\n",
    "# check no NaN values\n",
    "# month and year appear correct\n",
    "# month_year in YYYY-MM format\n",
    "# shipping delay positive\n",
    "\n",
    "# 2. Validate profit margin\n",
    "df[df['profit_margin'] > 1]\n",
    "\n",
    "# check for extreme negatives\n",
    "df[df['profit_margin'] < -1]\n",
    "# 349 rows, due to strong discounts\n",
    "\n",
    "# 3. Validate shipping delay\n",
    "df['shipping_delay'].describe()\n",
    "\n",
    "df[df['shipping_delay'] > 20]\n",
    "\n",
    "# 4. Validate month-year continuity\n",
    "df['order_month_year'].nunique()\n",
    "\n",
    "# Expected ~48 months\n",
    "\n",
    "# 5. Validate year and month distributions\n",
    "df['order_year'].value_counts()\n",
    "df['order_month'].value_counts().sort_index()\n",
    "# These should show regular patters, not anomalies\n",
    "\n",
    "# 6. Check for NaN in new columns\n",
    "df[['sales', 'profit', 'profit_margin', 'order_year', 'order_month', 'order_month_year', 'shipping_delay']].isna().sum()\n",
    "\n",
    "# Summary: ready for EDA (Exploratory Data Analysis)\n",
    "\n",
    "# I: Save cleaned dataset\n",
    "\n",
    "# Save cleaned and enriched dataset for future EDA and PowerBI\n",
    "\n",
    "# 1. Save as CSV\n",
    "df.to_csv('../data/superstore_clean.csv', index=False)\n",
    "# index have no business meaning and will pollute dataset\n",
    "\n",
    "# 2. Save as Excel\n",
    "df.to_excel('../data/superstore_clean.xlsx', index=False)\n",
    "\n",
    "# 3. Verify files\n",
    "# CSV\n",
    "df_test = pd.read_csv(\"../data/superstore_clean.csv\")\n",
    "df_test.head()\n",
    "\n",
    "# Excel\n",
    "df_test = pd.read_excel(\"../data/superstore_clean.xlsx\")\n",
    "df_test.head()\n",
    "\n",
    "# Make sure all columns present\n",
    "# Datatypes look correct\n",
    "# No duplicate header row\n",
    "# Row count matches your cleaned dataset\n",
    "\n",
    "# 4. Save a lightweight sample for PowerBI testing\n",
    "df.sample(200).to_csv('../data/superstore_clean_sample.csv', index=False)\n",
    "# This speeds up dashboarding prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334fda65",
   "metadata": {},
   "source": [
    "### Data Cleaning Summary\n",
    "\n",
    "### Date Columns\n",
    "\n",
    "- Converted 'order_date' and 'ship_date' to datetie format.\n",
    "- Sorted entire dataset by 'order_date' -> 'ship_date'.\n",
    "- Varified there are no invalid date values.\n",
    "\n",
    "### Duplicated and Missing Values\n",
    "\n",
    "- Found and removed **0** duplicate rows.\n",
    "- Check for missing values - no critical NaNs identified.\n",
    "\n",
    "### Numeric Consistency Checks\n",
    "\n",
    "- Quantity: all values positive.\n",
    "- Discount: all values from 0 to 0.8.\n",
    "- No values where profit exceeded sales.\n",
    "- All shipping dates occur after or the same date as order date.\n",
    "\n",
    "### Outliers Analysis\n",
    "\n",
    "- High discounts (> 50%) found primarily in Furniture category.\n",
    "- Several extremely negative profits (< -2000) observed - all result of high discounts.\n",
    "- Large sales values (> 5000) represent big furniture orders.\n",
    "- Profit distribution is heavily tailed with many negative cases.\n",
    "- Outliers will be kept for analysis, but noted in reporting.\n",
    "\n",
    "### New Features Created\n",
    "\n",
    "- 'profit_margin' = profit / sales\n",
    "- 'order_year' (2014-2017)\n",
    "- 'order_month' (1-12)\n",
    "- 'month_year' (Period, useful for monthly trends)\n",
    "- 'shipping_delay' in days\n",
    "\n",
    "### Output Files Saved\n",
    "- 'superstore_clean.csv' - cleaned dataset for EDA and BI\n",
    "- 'superstore_clean.xlsx' - cleaned excel version for PowerBI\n",
    "- Notebook updated with all cleaning steps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sales & Profit Overview\n",
    "\n",
    "# 1.1 High-Level KPI Summary\n",
    "\n",
    "# Goal: Get Basic financial metrics of the dataset\n",
    "\n",
    "total_sales = df['sales'].sum()\n",
    "total_profit = df['profit'].sum()\n",
    "total_quantity = df['quantity'].sum()\n",
    "avg_profit_margin = df['profit_margin'].mean()\n",
    "\n",
    "# print(f\"total_sales: {total_sales}\")\n",
    "# print(f\"total_profit: {total_profit}\")\n",
    "# print(f\"total_quantity: {total_quantity}\")\n",
    "# print(f\"avg_profit_margin: {avg_profit_margin}\")\n",
    "\n",
    "# Interpretation:\n",
    "# Total Sales: the total revenue\n",
    "# Total Profit: actual earnings after costs\n",
    "# Total Quantity: number of units sold\n",
    "# Average Profit Margin: overall business profitability\n",
    "\n",
    "# 1.2 Sales by Category\n",
    "# Goal: see which categories generate the most revenue\n",
    "\n",
    "sales_by_category = df.groupby('category')['sales'].sum().sort_values(ascending=False) \n",
    "# sales_by_category\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.barplot(\n",
    "#     x=sales_by_category.index,\n",
    "#     y=sales_by_category.values\n",
    "# )\n",
    "# plt.title(\"Sales by Category\")\n",
    "# plt.xlabel(\"Category\")\n",
    "# plt.ylabel(\"Total Sales\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Interpretation:\n",
    "# Technology has the highest revenue.\n",
    "# Furniture is second\n",
    "# Office Suppliers is third\n",
    "\n",
    "# Key question to answer: which categories deserve more investments, based on revenues\n",
    "\n",
    "# 1.3 Profit By Category \n",
    "# Profit tells a different story from Sales\n",
    "\n",
    "profit_by_category =  df.groupby('category')['profit'].sum().sort_values(ascending=False) \n",
    "profit_by_category\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.barplot(\n",
    "#     x=profit_by_category.index,\n",
    "#     y=profit_by_category.values\n",
    "# )\n",
    "# plt.title(\"Profit by Category\")\n",
    "# plt.xlabel(\"Category\")\n",
    "# plt.ylabel(\"Total Profit\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Interpretation:\n",
    "# Technology has the highest profit.\n",
    "# Office Suppliers is second\n",
    "# Furniture is third, with VERY LOW PROFIT\n",
    "\n",
    "# Question to Answer: are we selling products that bring revenue but don't bring profit\n",
    "\n",
    "\n",
    "# 1.4 Profit Margin by Category\n",
    "\n",
    "# Goal\n",
    "# Show how efficient every category is\n",
    "avg_profit_margin_by_category =  df.groupby('category')['profit_margin'].mean().sort_values(ascending=False) \n",
    "avg_profit_margin_by_category\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.barplot(\n",
    "#     x=avg_profit_margin_by_category.index,\n",
    "#     y=avg_profit_margin_by_category.values\n",
    "# )\n",
    "# plt.title(\"Avg Profit Margin by Category\")\n",
    "# plt.xlabel(\"Category\")\n",
    "# plt.ylabel(\"Avg Profit Margin\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# Categories with high sales but low margin is risky (Furniture)\n",
    "# Categories with low sales but high margin can be opportunity\n",
    "\n",
    "# 1.5 Sales and profit by Sub-Category\n",
    "\n",
    "# Goal\n",
    "# Deepen the analysis: which product types drive results\n",
    "\n",
    "sub_metrics = (\n",
    "    df.groupby('sub-category')[['sales', 'profit']]\n",
    "      .sum()\n",
    "      .sort_values('profit', ascending=False)   # optional sorting\n",
    ")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sub_metrics.plot(kind='bar', figsize=(12,6))\n",
    "# plt.title('Sales & Profit by Sub-Category')\n",
    "# plt.xlabel('Sub-Category')\n",
    "# plt.ylabel('Amount')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# High sales but low profit: Phones, Chairs, Storage\n",
    "# Loss making sub-categories: Tables, Bookcases, Supplies\n",
    "# Hidden gems with high margin: Paper, Copiers, Accessories, Envelopes, Labels\n",
    "\n",
    "# 1.6 Sales & Profit By Segment\n",
    "# Identify, which custommer segment is most valuable\n",
    "\n",
    "sub_metrics = (\n",
    "    df.groupby('segment')[['sales', 'profit']]\n",
    "      .sum()\n",
    "      .sort_values('profit', ascending=False)   # optional sorting\n",
    ")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sub_metrics.plot(kind='bar', figsize=(12,6))\n",
    "# plt.title('Sales & Profit by Segment')\n",
    "# plt.xlabel('Segment')\n",
    "# plt.ylabel('Amount')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# Consumer has the highest revenue.\n",
    "# Corporate also has strong profit.\n",
    "# Home Office is the smallest but highly profitable.\n",
    "\n",
    "# Key business questions:\n",
    "# Which segment is growing or shrinking?\n",
    "# Should marketing focus on Consumer or Corporate clients?\n",
    "\n",
    "# 1.7. Combined Sales & Profit into a Single Overview Table\n",
    "\n",
    "overview = df.groupby('category').agg({\n",
    "    'sales': 'sum',\n",
    "    'profit': 'sum',\n",
    "    'profit_margin': 'mean'\n",
    "}).sort_values(by='profit', ascending=False)\n",
    "\n",
    "print(overview)\n",
    "\n",
    "overview = df.groupby('segment').agg({\n",
    "    'sales': 'sum',\n",
    "    'profit': 'sum',\n",
    "    'profit_margin': 'mean'\n",
    "}).sort_values(by='profit', ascending=False)\n",
    "\n",
    "print(overview)\n",
    "\n",
    "# Will be added to README\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a1428",
   "metadata": {},
   "source": [
    "- **Technology** leads in both sales and profit, indicating strong demand and good margins.\n",
    "\n",
    "- **Furniture** generates high revenue but weak profit, suggesting pricing or cost issues.\n",
    "\n",
    "- Several sub-categories (e.g., **Tables**, **Bookcases**, **Supplies**) show negative profit despite strong sales.\n",
    "\n",
    "- The **Consumer** segment is the largest revenue driver, but Corporate appears more efficient.\n",
    "**Home Office** has the lowest Sales but the biggest profit margin \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a394ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Product Performance Analysis\n",
    "# In this section we will analyze individual performance to identify\n",
    "\n",
    "# - Best selling product\n",
    "# - Most profitable product\n",
    "# - Products that cause financial loss\n",
    "# - Profit margin distribution across products\n",
    "\n",
    "# 2.1. Top 10 Products by Sales\n",
    "# Goal: find products generating the highest revenue.\n",
    "\n",
    "top_sales = (\n",
    "    df.groupby(['category', 'product_name'])['sales']\n",
    "      .sum()\n",
    "      .sort_values(ascending=False)\n",
    "      .head(10)\n",
    ")\n",
    "# print(top_sales)\n",
    "\n",
    "# Prepare data for plot\n",
    "top_sales_plot = top_sales.copy()\n",
    "top_sales_plot.index = top_sales_plot.index.map(lambda x: f\"{x[0]} — {x[1]}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(\n",
    "#     x=top_sales_plot.values,\n",
    "#     y=top_sales_plot.index\n",
    "# )\n",
    "# plt.title(\"Top 10 Products by Sales\")\n",
    "# plt.xlabel(\"Total Sales\")\n",
    "# plt.ylabel(\"Product Category + Name\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# Technologies and Office Suppliers dominate revenue.\n",
    "# Leads 'Technology - Canon imageCLASS 2200 Advanced Copier' with a big gap  \n",
    "\n",
    "# 2.2. Top 10 Products by Profit\n",
    "# Goal: identify items that actually generate earnings.\n",
    "\n",
    "top_profit = (\n",
    "    df.groupby(['category','product_name'])['profit']\n",
    "      .sum()\n",
    "      .sort_values(ascending=False)\n",
    "      .head(10)\n",
    ")\n",
    "\n",
    "# Prepare data for plot\n",
    "top_profit_plot = top_profit.copy()\n",
    "top_profit_plot.index = top_profit_plot.index.map(lambda x: f\"{x[0]} — {x[1]}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(\n",
    "#     x=top_profit_plot.values,\n",
    "#     y=top_profit_plot.index\n",
    "# )\n",
    "# plt.title(\"Top 10 Products by Profit\")\n",
    "# plt.xlabel(\"Total Profit\")\n",
    "# plt.ylabel(\"Product Category + Name\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# Technology category dominate profit.\n",
    "# Office supliers follow with big gap.\n",
    "# Leads 'Technology - Canon imageCLASS 2200 Advanced Copier' with a big gap  \n",
    "\n",
    "# 2.3. Bottom 10 Products by Profit (Worst Performers)\n",
    "# Goal: reveal products that consistently lose money.\n",
    "\n",
    "bottom_profit = (\n",
    "    df.groupby(['category', 'product_name'])['profit']\n",
    "      .sum()\n",
    "      .sort_values()\n",
    "      .head(10)\n",
    ")\n",
    "\n",
    "# Prepare data for plot\n",
    "bottom_profit_plot = bottom_profit.copy()\n",
    "bottom_profit_plot.index = bottom_profit_plot.index.map(lambda x: f\"{x[0]} — {x[1]}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(\n",
    "#     x=bottom_profit_plot.values,\n",
    "#     y=bottom_profit_plot.index\n",
    "# )\n",
    "# plt.title(\"Bottom 10 Products by Profit (Most Unprofitable)\")\n",
    "# plt.xlabel(\"Total Profit\")\n",
    "# plt.ylabel(\"Product Category + Name\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "# Most losses come from high-cost Technology products (3D printers, laser printers, conferencing systems).\n",
    "# Several bulky Furniture items (conference tables) are also strongly unprofitable.\n",
    "# A few Office Supplies products appear, indicating potential pricing or overstock issues.\n",
    "# Top three items show very large negative profits, far worse than others.\n",
    "\n",
    "# Key question:\n",
    "# Why are we selling products that consistently produce losses?\n",
    "\n",
    "# 2.4. Profit Margin Distribution Across Products\n",
    "# Goal: understand how profitable individual products are on average.\n",
    "\n",
    "pm_distribution = (\n",
    "    df.groupby(['category','product_name'])['profit_margin']\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "pm_distribution.describe()\n",
    "\n",
    "# Visualization: Histogram\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.histplot(pm_distribution, bins=30, kde=True)\n",
    "# plt.title(\"Distribution of Average Profit Margin Across Products\")\n",
    "# plt.xlabel(\"Average Profit Margin\")\n",
    "# plt.ylabel(\"Count of Products\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "# Most products have positive profit margins, clustered between 0% and 40%.\n",
    "# A noticeable group of products has low or near-zero margins, indicating weak profitability.\n",
    "# A small but important tail of products shows strongly negative margins, meaning they lose money.\n",
    "# The distribution is right-skewed, with many moderately profitable items and few extreme losses.\n",
    "# Negative-margin products should be reviewed for pricing, discounts, or high return rates.\n",
    "\n",
    "\n",
    "# 2.5. Identify “High Sales but Low Profit” Products\n",
    "\n",
    "# Goal: find items that sell well but don’t bring profit — a major business risk.\n",
    "\n",
    "product_stats = df.groupby(['category', 'product_name']).agg({\n",
    "    'sales': 'sum',\n",
    "    'profit': 'sum',\n",
    "    'profit_margin': 'mean'\n",
    "})\n",
    "\n",
    "product_stats.index = product_stats.index.map(lambda x: f\"{x[0]} — {x[1]}\")\n",
    "\n",
    "high_sales_low_profit = (\n",
    "    product_stats[(product_stats['sales'] > product_stats['sales'].median()) & \n",
    "                  (product_stats['profit'] < 0)]\n",
    "    .sort_values(by='sales', ascending=False)\n",
    ")\n",
    "\n",
    "high_sales_low_profit.head(10)\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "# These products:\n",
    "# Generate strong revenue\n",
    "# But damage total profit\n",
    "# Usually tied to large discounts or high shipping cost items\n",
    "# Business action: review pricing or discontinue.\n",
    "\n",
    "\n",
    "# 2.6. Product-Level Correlation Check\n",
    "# Goal: check if sales and profit correlate at product level.\n",
    "\n",
    "product_corr = product_stats[['sales', 'profit']].corr()\n",
    "# print(product_corr)\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "# The correlation (0.59) is moderately positive but not perfect.\n",
    "# This confirms that high sales ≠ high profit.\n",
    "\n",
    "# 2.7. Summary Table of Key Product Metrics\n",
    "\n",
    "# This table is perfect for future dashboard/report use.\n",
    "\n",
    "product_overview = product_stats.sort_values(by='profit', ascending=False).head(15)\n",
    "product_overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23c9c4",
   "metadata": {},
   "source": [
    "### Product Performance Analysis\n",
    "- Technologies and Office Suppliers dominate revenue.\n",
    "- Technology category dominate profit & Office supliers follow with big gap.\n",
    "- Most losses come from high-cost Technology products (3D printers, laser printers, conferencing systems). Several bulky Furniture items (conference tables) are also strongly unprofitable.\n",
    "- Most products have positive profit margins, clustered between 0% and 40%.  A noticeable group of products has low or near-zero margins, indicating weak profitability.\n",
    "- The correlation (0.59) between sales and profit is moderately positive but not perfect. This confirms that high sales ≠ high profit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f847f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Customer Performance Analysis\n",
    "\n",
    "# Customers drive revenue and profit.In this section we analyze:\n",
    "# Top customers by sales\n",
    "# Top customers by profit\n",
    "# Most unprofitable customers\n",
    "# Customer segmentation (high/middle/low value)\n",
    "# Profit margin by Segment\n",
    "\n",
    "# Goal: understand which customers contribute most to business and which customers may cause losses due to returns, high discounts, or heavy shipping costs.\n",
    "\n",
    "# 3.1 Top 10 customers by sales\n",
    "\n",
    "top_customers_sales = df.groupby(['segment', 'customer_name'])['sales'].sum().sort_values(ascending=False).head(10)\n",
    "top_customers_sales.index = top_customers_sales.index.map(lambda x: f\"{x[0]} — {x[1]}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(\n",
    "#     x=top_customers_sales.values,\n",
    "#     y=top_customers_sales.index\n",
    "# )\n",
    "# plt.title(\"Top 10 Customers by Sales\")\n",
    "# plt.xlabel(\"Total Sales\")\n",
    "# plt.ylabel(\"Customer Segment + Name\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# The top customer by sales is Sean Miller (Home Office), far ahead of all others.\n",
    "# Corporate segment customers appear less frequently but generate very high order values, represented by Tamara Chand.\n",
    "# Most top customers belong to the Consumer segment, but their spending is more evenly distributed.\n",
    "\n",
    "# 3.2 Top 10 customers by profit\n",
    "\n",
    "top_customers_profit = df.groupby(['segment', 'customer_name'])['profit'].sum().sort_values(ascending=False).head(10)\n",
    "top_customers_profit.index = top_customers_profit.index.map(lambda x: f\"{x[0]} — {x[1]}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(\n",
    "#     x=top_customers_profit.values,\n",
    "#     y=top_customers_profit.index\n",
    "# )\n",
    "# plt.title(\"Top 10 Customers by Profit\")\n",
    "# plt.xlabel(\"Total Profit\")\n",
    "# plt.ylabel(\"Customer Segment + Name\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# Corporate customer, Tamara Chand, is the clear top contributor.\n",
    "# The top customer by sales, Sean Miller (Home Office), is NOT in the list \n",
    "# Most other top-profit customers come from the Consumer segment, with strong but more evenly distributed profit contributions.\n",
    "# Home Office customers appear less frequently but still include solid profit generators.\n",
    "# High-profit customers are spread across all segments, indicating broad profitability rather than reliance on a single segment.\n",
    "\n",
    "# 3.3 Bottom 10 customers by profit (Loss-Makers)\n",
    "\n",
    "bottom_customers_profit = df.groupby(['segment', 'customer_name'])['profit'].sum().sort_values().head(10)\n",
    "bottom_customers_profit.index = bottom_customers_profit.index.map(lambda x: f\"{x[0]} — {x[1]}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(\n",
    "#     x=bottom_customers_profit.values,\n",
    "#     y=bottom_customers_profit.index\n",
    "# )\n",
    "# plt.title(\"Bottom 10 Customers by Profit (Highest Losses)\")\n",
    "# plt.xlabel(\"Total Profit\")\n",
    "# plt.ylabel(\"Customer Segment + Name\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# The largest loss-maker is Cindy Stewart (Consumer) with a significantly higher negative impact than all others.\n",
    "# Loss-making customers appear across all segments (Consumer, Corporate, Home Office), showing it’s not isolated to one segment. BUT\n",
    "# Mostly Corporate and a few Home Office customers contribute substantial losses, suggesting pricing, discounting, or shipping cost issues for certain orders.\n",
    "\n",
    "# 3.4. Customer Segmentation (High / Mid / Low Value)\n",
    "\n",
    "# Goal: segment customers by total profit or revenue\n",
    "# Classic segmentation = Pareto 20/60/20 rule.\n",
    "\n",
    "customers_profit = df.groupby('customer_name')['profit'].sum().sort_values(ascending=False)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "high_treshold = np.percentile(customers_profit, 80)\n",
    "low_treshold = np.percentile(customers_profit, 20)\n",
    "\n",
    "def segment_customer(p):\n",
    "    if p >= high_treshold:\n",
    "        return 'High Value'\n",
    "    elif p <= low_treshold:\n",
    "        return 'Low Value'\n",
    "    else:\n",
    "        return 'Mid Value'\n",
    "    \n",
    "customers_segmentation = customers_profit.apply(segment_customer)\n",
    "customers_segmentation.value_counts()\n",
    "\n",
    "# Interpretation\n",
    "# Mid-Value customers are the largest group (475 customers), representing the core of the customer base. They generate steady but moderate profit and form the bulk of recurring business.\n",
    "\n",
    "# High-Value customers (159 customers) are a much smaller group but likely contribute a disproportionately large share of total profit. These are the most profitable and strategically important customers.\n",
    "\n",
    "# Low-Value customers (159 customers) form the smallest—but important—segment. Many of them produce low or negative profit and may require pricing, discount, or service-cost optimization.\n",
    "\n",
    "# 3.5 Merge Segmentation Back to the Data\n",
    "# Analyze average behaviour per segment\n",
    "\n",
    "df_customers = df.copy()\n",
    "df_customers['customer_segment'] = df_customers['customer_name'].map(customers_segmentation)\n",
    "\n",
    "\n",
    "# 3.6 Profit Margin by Customer Segment\n",
    "# Goal: understand which customer segment is most efficient\n",
    "\n",
    "customer_segment_pm = df_customers.groupby('customer_segment')['profit_margin'].mean().sort_values(ascending=False)\n",
    "# print(customer_segment_pm)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(7,4))\n",
    "# sns.barplot(\n",
    "#     x=customer_segment_pm.index,\n",
    "#     y=customer_segment_pm.values\n",
    "# )\n",
    "# plt.title(\"Average Profit Margin by Customer Segment\")\n",
    "# plt.xlabel(\"Customer Segment\")\n",
    "# plt.ylabel(\"Avg Profit Margin\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "# High Value customers generate high margins (0.188).\n",
    "# Low Value customers may generate negative margins (-0.028).\n",
    "# Mid Value customers show stable but average profitability (0.136).\n",
    "\n",
    "# 3.7 Summary Table of Segment-Level Metrics\n",
    "\n",
    "customer_segment_metrics = df_customers.groupby('customer_segment').agg({\n",
    "    'sales': 'sum',\n",
    "    'profit': 'sum',\n",
    "    'profit_margin': 'mean',\n",
    "    'customer_name': 'nunique'\n",
    "}).rename(columns={'customer_name': 'unique_customers'})\n",
    "customer_segment_metrics\n",
    "\n",
    "# High-Value customers (159 customers) generate the largest total profit (233.885 $) and the highest profit margin (18.8%), making them the most valuable segment for the business.\n",
    "\n",
    "# Mid-Value customers (475 customers) form the majority of the customer base and contribute strong (123.734 $), stable revenue with healthy profit margins (13.6%).\n",
    "\n",
    "# Low-Value customers (159 customers) generate substantial revenue but produce negative total profit (-71.222 $), indicating heavy discounts, high costs, or unprofitable buying behavior.\n",
    "\n",
    "# Overall, profitability is concentrated in the High-Value and Mid-Value groups, while the Low-Value segment represents a clear opportunity for pricing or cost optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80152eb9",
   "metadata": {},
   "source": [
    "### Customer Performance Analysis\n",
    "\n",
    "- Most top customers belong to the Consumer segment, but their spending is more evenly distributed.\n",
    "- Most top-profit customers come from the Consumer segment, with strong but more evenly distributed profit contributions. Home Office customers appear less frequently but still include solid profit generators.\n",
    "- Loss-making customers appear across all segments (Consumer, Corporate, Home Office), showing it’s not isolated to one segment. BUT Mostly Corporate and a few Home Office customers contribute substantial losses.\n",
    "- Overall, profitability is concentrated in the High-Value and Mid-Value groups, while the Low-Value segment represents a clear opportunity for pricing or cost optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950ad4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Time Series Trends\n",
    "\n",
    "# Understanding how sales and profit evolve over time helps identify seasonality, growth patterns, and anomalies that can guide business decisions.\n",
    "# In this part of EDA, we analyze:\n",
    "\n",
    "# Yearly trends\n",
    "# Monthly trends\n",
    "# Month-Year timeline\n",
    "# Sales vs Profit comparison over time\n",
    "# Seasonal peaks or unusual dips\n",
    "# This analysis also prepares the foundation for dashboard visualizations and YoY insights.\n",
    "\n",
    "# 4.1 Prepare Time-Based Columns (if not already done)\n",
    "# Goal: ensure the dataset contains time-specific features needed for analysis.\n",
    "\n",
    "# if already done before, just confirm\n",
    "# df['order_year'] = df['Order Date'].dt.year\n",
    "# df['order_month'] = df['Order Date'].dt.month\n",
    "# df['month_year'] = df['order_month_year'].dt.to_period('M').astype(str)\n",
    "\n",
    "# df[['Order Date', 'order_year', 'order_month', 'month_year']].head()\n",
    "\n",
    "# I already have needed columns\n",
    "\n",
    "# 4.2 Yearly Sales and Profit Trends\n",
    "\n",
    "# Goal: get a high-level view of annual business performance.\n",
    "\n",
    "yearly = df.groupby('order_year').agg({\n",
    "    'sales': 'sum',\n",
    "    'profit': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# sns.lineplot(data=yearly, x='order_year', y='sales', marker='o', label='Sales')\n",
    "# sns.lineplot(data=yearly, x='order_year', y='profit', marker='o', label='Profit')\n",
    "# plt.title(\"Yearly Sales & Profit Trends\")\n",
    "# plt.xlabel(\"Year\")\n",
    "# plt.ylabel(\"Amount ($)\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "# Both sales and profit show a clear upward trend from 2014 to 2017, indicating strong overall business growth.\n",
    "# After a slight dip in 2015, sales accelerate sharply in 2016 and 2017, suggesting improved demand or successful sales strategies.\n",
    "# Profit increases steadily every year, growing faster than sales in percentage terms, which indicates improving efficiency or margin management.\n",
    "\n",
    "# 4.3. Monthly Sales & Profit Trend (Seasonality)\n",
    "# Goal: identify seasonal patterns (strong months, slow months).\n",
    "\n",
    "monthly = df.groupby('order_month').agg({\n",
    "    'sales': 'sum',\n",
    "    'profit': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# sns.lineplot(data=monthly, x='order_month', y='sales', marker='o', label='Sales')\n",
    "# sns.lineplot(data=monthly, x='order_month', y='profit', marker='o', label='Profit')\n",
    "# plt.title(\"Monthly Sales Trend (Seasonality)\")\n",
    "# plt.xlabel(\"Month\")\n",
    "# plt.ylabel(\"Total Sales\")\n",
    "# plt.xticks(range(1,13))\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "# The business shows clear seasonality, with strong peaks in March, September, November, and December, indicating high-demand periods.\n",
    "# Sales dip noticeably in January, February, and April, marking recurring low-demand periods.\n",
    "# Profit follows the same pattern as sales, showing consistent growth during peak months and smaller gains during slow periods.\n",
    "\n",
    "# 4.4. Month-Year Timeline (Full Time Series)\n",
    "# Goal: visualize a continuous timeline (e.g., from 2014–2017).\n",
    "\n",
    "order_month_year_trend = (\n",
    "    df.groupby('order_month_year')[['sales', 'profit']]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# order_month_year_trend.head()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "# sns.lineplot(data=order_month_year_trend, x='order_month_year', y='sales', label='Sales')\n",
    "# sns.lineplot(data=order_month_year_trend, x='order_month_year', y='profit', label='Profit')\n",
    "# plt.title(\"Sales & Profit Over Time (Month-Year)\")\n",
    "# plt.xlabel(\"Month-Year\")\n",
    "# plt.ylabel(\"Amount ($)\")\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "# Both sales and profit show strong upward momentum over multiple years, with noticeably higher peaks toward late 2016 and throughout 2017.\n",
    "# The chart clearly displays monthly seasonality, with recurring spikes each year—especially in Q4 (Oct–Dec) and occasional peaks in Q1/Q2.\n",
    "# Profit closely follows sales patterns but with much smaller magnitude, confirming a consistent relationship between revenue and profitability.\n",
    "\n",
    "# 4.5. Optional: Year-over-Year Growth (YoY)\n",
    "# Goal: compute annual growth to highlight acceleration or decline.\n",
    "\n",
    "# YoY Growth (%) = (Current Year - Previous Year) / Previous Year * 100\n",
    "\n",
    "yearly['sales_yoy'] = yearly['sales'].pct_change() * 100\n",
    "yearly['profit_yoy'] = yearly['profit'].pct_change() * 100\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "# 2015: Sales dipped slightly (–2.8% YoY), but profit grew strongly (+24.4%), showing improved margins despite flat revenue.\n",
    "# 2016: Both metrics accelerated, with sales up +29.5% and profit up +32.7%, indicating a major growth year with strong operational efficiency.\n",
    "# 2017: Growth continued, though at a slower pace—sales grew +20.3% and profit increased +14.2%, showing stable expansion.\n",
    "# Overall, YoY results show consistent improvement, with profitability growing alongside revenue, and particularly strong performance in 2016.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2340a4a",
   "metadata": {},
   "source": [
    "### Time Series Trends Analysis\n",
    "\n",
    "- The business shows clear seasonality, with strong peaks in March, September, November, and December, indicating high-demand periods. Profit follows the same pattern as sales, showing consistent growth during peak months and smaller gains during slow periods.\n",
    "- Both sales and profit show strong upward momentum over multiple years, with noticeably higher peaks toward late 2016 and throughout 2017. Profit closely follows sales patterns but with much smaller magnitude, confirming a consistent relationship between revenue and profitability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26928b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Discount Impact Analysis\n",
    "\n",
    "# Discounts directly influence sales volume and profit margins.\n",
    "\n",
    "# In this section, we examine:\n",
    "# How discounts relate to profit\n",
    "# Whether higher discounts destroy profitability\n",
    "# At what discount level profit becomes negative\n",
    "# How discount behavior varies by category\n",
    "# This helps identify pricing and promotion issues.\n",
    "\n",
    "# 5.1. Scatterplot: Discount vs Profit\n",
    "# Goal: visualize the relationship between discount percentage and profit on each order.\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.scatterplot(\n",
    "#     data=df,\n",
    "#     x='discount',\n",
    "#     y='profit',\n",
    "#     alpha=0.4\n",
    "# )\n",
    "# plt.title(\"Discount vs Profit\")\n",
    "# plt.xlabel(\"Discount\")\n",
    "# plt.ylabel(\"Profit\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "# Higher discounts are strongly associated with lower profit, and many high-discount transactions produce negative profit.\n",
    "# The highest profits occur almost exclusively at 0%–10% discount, showing that full-price or lightly discounted sales drive profitability.\n",
    "# As discount levels rise (40%–80%), profit values cluster close to zero or deep into negative territory, indicating unprofitable discounting practices.\n",
    "\n",
    "# 5.2. Correlation Between Discount and Profit / Profit Margin\n",
    "# Goal: quantify how strong the relationship is.\n",
    "\n",
    "df[['discount', 'profit', 'profit_margin']].corr()\n",
    "\n",
    "# Interpretation:\n",
    "# Discount has a strong negative correlation with profit margin (–0.86), meaning higher discounts almost always lead to significantly lower margins.\n",
    "# There is also a moderate negative correlation between discount and total profit (–0.22), showing that discounts generally reduce profit rather than driving high-profit sales.\n",
    "# Profit and profit margin show a weak positive correlation (+0.22), consistent with the idea that higher margins tend to create higher profit, but many other factors affect profit too.\n",
    "# Overall, the correlations confirm that aggressive discounting is harmful to profitability, both in absolute profit and percentage margins.\n",
    "\n",
    "\n",
    "# 5.3. Create Discount Bins (0–10%, 10–20%, …)\n",
    "# Goal: group orders into discount ranges to see profitability at each level.\n",
    "\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 1]\n",
    "labels = ['0-10%', '10-20%', '20-30%','30-40%','40%+']\n",
    "\n",
    "df['discount_bin'] = pd.cut(df['discount'], bins=bins, labels=labels, include_lowest=True)\n",
    "df['discount_bin'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Interpretation\n",
    "# Most orders fall into low-discount ranges (0–10% and 10–20%), together making up the majority of all transactions.\n",
    "\n",
    "# 5.4. Profit by Discount Bin\n",
    "# Goal: understand how profit behaves at different discount levels.\n",
    "\n",
    "profit_by_bin = df.groupby('discount_bin')['profit'].sum().sort_index()\n",
    "# profit_by_bin\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.barplot(\n",
    "#     x=profit_by_bin.index,\n",
    "#     y=profit_by_bin.values\n",
    "# )\n",
    "# plt.title(\"Total Profit by Discount Range\")\n",
    "# plt.xlabel(\"Discount Range\")\n",
    "# plt.ylabel(\"Total Profit\")\n",
    "# plt.show()\n",
    "\n",
    "# 0–10% discounts generate the overwhelming majority of total profit, making low-discount sales the core profit driver of the business.\n",
    "# Profit drops sharply at 10–20% discounts, though this range still remains profitable overall.\n",
    "# All discount ranges above 20% produce negative total profit, with losses increasing as discounts deepen.\n",
    "# The 40%+ discount range shows the largest losses, confirming that deep discounting is highly unprofitable.\n",
    "# Overall, the chart clearly demonstrates that profitability declines non-linearly with higher discounts, emphasizing the need for strict discount controls.\n",
    "\n",
    "# 5.5. Average Profit Margin by Discount Bin\n",
    "# Goal: see how efficient each discount level is.\n",
    "\n",
    "pm_by_bin = df.groupby('discount_bin')['profit_margin'].mean().sort_values()\n",
    "pm_by_bin\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.barplot(\n",
    "#     x=pm_by_bin.index,\n",
    "#     y=pm_by_bin.values\n",
    "# )\n",
    "# plt.title(\"Average Profit Margin by Discount Range\")\n",
    "# plt.xlabel(\"Discount Range\")\n",
    "# plt.ylabel(\"Avg Profit Margin\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# Low discounts (0–10%) deliver the highest average profit margin, making them the most financially healthy pricing range.\n",
    "# Margins decline steadily as discounts increase, with 10–20% still positive but significantly lower.\n",
    "# From 20–40% discount, average margins turn negative, showing that these sales generally lose money.\n",
    "# The 40%+ discount range has extremely negative margins, confirming that deep discounting is highly unprofitable.\n",
    "# Overall, the margin trend reinforces that higher discounts directly decrease profitability, with a clear break-even point around the 20% discount level.\n",
    "\n",
    "# 5.6. Category-Level Discount Behavior\n",
    "# Goal: identify categories that depend heavily on discounting.\n",
    "\n",
    "category_discount = df.groupby('category')['discount'].mean().sort_values(ascending=False)\n",
    "category_discount\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.barplot(\n",
    "#     x=category_discount.values,\n",
    "#     y=category_discount.index\n",
    "# )\n",
    "# plt.title(\"Average Discount by Category\")\n",
    "# plt.xlabel(\"Average Discount\")\n",
    "# plt.ylabel(\"Category\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# Furniture has the highest average discount, indicating more aggressive markdowns or frequent promotions in this category.\n",
    "# Office Supplies receive moderate discounts, suggesting competitive pricing or regular small markdowns.\n",
    "# Technology has the lowest average discount, implying stronger pricing power and less need for promotional reductions.\n",
    "# Overall, discounting varies significantly by category, with Furniture being most discount-sensitive and Technology being the most price-stable.\n",
    "\n",
    "# 5.7 Identify Order with High Discounts and Negative Profit\n",
    "\n",
    "high_discounts_losses = df[(df['discount'] > 0.3) & (df['profit'] < 0)]\n",
    "high_discounts_losses\n",
    "\n",
    "# 1. Quantify the problem\n",
    "num_orders = len(high_discounts_losses) #1140\n",
    "total_loss = high_discounts_losses['profit'].sum() #-127737.555\n",
    "avg_loss = high_discounts_losses['profit'].mean() #112.05\n",
    "\n",
    "# 2. Analyze which categories cause the most damage\n",
    "\n",
    "high_discounts_losses['category'].value_counts()\n",
    "# Office Supplies    680\n",
    "# Furniture          320\n",
    "# Technology         140\n",
    "high_discounts_losses.groupby('category')['profit'].sum().sort_values()\n",
    "# Office Supplies   -47140.1376\n",
    "# Furniture         -43782.4392\n",
    "# Technology        -36814.9782\n",
    "\n",
    "# 3. Analyze which products are driving the losses\n",
    "high_discounts_losses.groupby('product_name')['profit'].sum().sort_values().head()\n",
    "# Cubify CubeX 3D Printer Double Head Print          -9239.9692\n",
    "# GBC DocuBind P400 Electric Binding System          -6859.3896\n",
    "# Lexmark MX611dhe Monochrome Laser Printer          -5269.9690\n",
    "# GBC Ibimaster 500 Manual ProClick Binding System   -5098.5660\n",
    "# GBC DocuBind TL300 Electric Binding System         -4162.0336\n",
    "\n",
    "# Interpretation:\n",
    "# A total of 1,140 orders had discounts above 30% and resulted in negative profit, creating a combined loss of $127,737 (avg. loss $112 per order).\n",
    "# Office Supplies (680 orders) and Furniture (320 orders) account for the majority of loss-making transactions, with total losses of –$47,140 and –$43,783, respectively.\n",
    "# The worst-performing products include Cubify 3D Printers, GBC binding machines, and Lexmark printers, each generating several thousand dollars of losses when sold with high discounts.\n",
    "# Overall, discounting varies significantly by category, with Furniture being most discount-sensitive and Technology being the most price-stable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c0f96",
   "metadata": {},
   "source": [
    "### Discount Impact Analysis\n",
    "\n",
    "- Discounts have a strong negative effect on profitability, with a –0.86 correlation between discount and profit margin.\n",
    "- Low discounts (0–10%) generate the highest profit and healthiest margins.\n",
    "- Profitability drops sharply above 20%, and discounts over 30% consistently generate losses.\n",
    "- Furniture receives the highest average discounts, followed by Office Supplies; Technology is discounted the least.\n",
    "- High-discount loss analysis shows 1,140 orders with discounts over 30% resulted in $127K total losses, mostly in Office Supplies and Furniture.\n",
    "- Several high-cost items (3D printers, binding machines, printers) are major loss drivers when heavily discounted.\n",
    "\n",
    "Conclusion:\n",
    "- Deep discounting is highly unprofitable. The business should limit discounts above 20–30% and review pricing strategies for high-cost, discount-sensitive products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic Analysis\n",
    "\n",
    "# Compare sales and profit by regions\n",
    "# Analyze profit margin by region\n",
    "# Identify the best and the worst regions by profit\n",
    "# Explore the relationship between category and region (heatmap)\n",
    "\n",
    "# 6.1 Sales and Profit by Region\n",
    "\n",
    "# Goal: to see which regions generate the highest sales and which are the most profitable\n",
    "\n",
    "region_perf = (\n",
    "    df.groupby('region').agg({\n",
    "        'sales': 'sum',\n",
    "        'profit': 'sum',\n",
    "        'profit_margin': 'mean'\n",
    "    })\n",
    "    .sort_values(by='profit',ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,4))\n",
    "# sns.barplot(\n",
    "#     data=region_perf.sort_values('sales', ascending=False),\n",
    "#     x='sales',\n",
    "#     y='region',\n",
    "#     label='Sales'\n",
    "# )\n",
    "# sns.barplot(\n",
    "#     data=region_perf.sort_values('sales', ascending=False),\n",
    "#     x='profit',\n",
    "#     y='region',\n",
    "#     label='Profit'\n",
    "# )\n",
    "# plt.title(\"Sales & Profit by Region\")\n",
    "# plt.xlabel(\"Total Sales & Profit\")\n",
    "# plt.ylabel(\"Region\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "### Interpretation\n",
    "# - West is the strongest region, leading in both total sales and total profit, with the highest profit margin (≈21.9%).\n",
    "# - East performs well with high sales and solid profit, maintaining a healthy profit margin (~16.7%).\n",
    "# - South generates lower sales but still remains profitable, with margins similar to East.\n",
    "# - Central is the only region operating at a negative profit margin (–10.4%), indicating pricing, cost, or discounting issues specific to this region.\n",
    "# - Conclusion: West and East are high-performing regions, while Central requires deeper review to understand and fix its loss-making behavior.\n",
    "\n",
    "# 6.2 Profit Margin by Region\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(7,4))\n",
    "# sns.barplot(\n",
    "#     data=region_perf.sort_values('profit_margin', ascending=False),\n",
    "#     x='profit_margin',\n",
    "#     y='region'\n",
    "# )\n",
    "# plt.title(\"Average Profit Margin by Region\")\n",
    "# plt.xlabel(\"Average Profit Margin\")\n",
    "# plt.ylabel(\"Region\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# West has the highest profit margin, making it the most profitable region.\n",
    "# East and South show solid, positive margins, indicating healthy performance.\n",
    "# Central is the only region with a negative profit margin, signaling operational or pricing issues that require attention.\n",
    "\n",
    "# 6.3\n",
    "# Sales and Profit by States\n",
    "# Goal: drill down to states to see where the profit comes and where it's lost\n",
    "\n",
    "states_perf = (\n",
    "    df.groupby('state').agg({\n",
    "        'sales': 'sum',\n",
    "        'profit': 'sum',\n",
    "        'profit_margin': 'mean'\n",
    "    })\n",
    "    .sort_values(by='profit',ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "# states_perf.head(10)\n",
    "\n",
    "top_states_perf = states_perf.head(10)\n",
    "\n",
    "# Visualization: Top 10 States by Profit\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(\n",
    "#     data=top_states_perf,\n",
    "#     x='profit',\n",
    "#     y='state'\n",
    "# )\n",
    "# plt.title(\"Top 10 States by Profit\")\n",
    "# plt.xlabel(\"Total Profit\")\n",
    "# plt.ylabel(\"State\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# California and New York are by far the most profitable states, each contributing the largest share of total profit.\n",
    "# Washington and Michigan also perform well, forming a strong second tier of profitable states.\n",
    "# States like Virginia, Indiana, and Georgia deliver moderate but solid profit levels.\n",
    "# Kentucky, Minnesota, and Delaware appear at the lower end but still contribute positively.\n",
    "# Conclusion: Profitability is highly concentrated in a few key states, with California and New York being the primary drivers of regional financial performance.\n",
    "\n",
    "bottom_states_perf = states_perf.sort_values('profit').head(10)\n",
    "\n",
    "# Visualization: Bottom 10 States by Profit\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(\n",
    "#     data=bottom_states_perf,\n",
    "#     x='profit',\n",
    "#     y='state'\n",
    "# )\n",
    "# plt.title(\"Bottom 10 States by Profit\")\n",
    "# plt.xlabel(\"Total Profit\")\n",
    "# plt.ylabel(\"State\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# Texas is the largest loss-making state by a wide margin, followed by Ohio and Pennsylvania, indicating major profitability issues in these markets.\n",
    "# Illinois and North Carolina also generate substantial losses, forming a second tier of underperforming states.\n",
    "# States like Colorado, Tennessee, Arizona, Florida, and Oregon show smaller but still negative profit levels.\n",
    "# The trend suggests that losses are concentrated in a handful of key states, especially Texas, which requires deeper investigation into pricing, discounting, product mix, or operational costs.\n",
    "# Conclusion: Losses are heavily concentrated in specific states—most notably Texas—highlighting the need for targeted corrective strategies in these regions.\n",
    "\n",
    "# 6.4 Region vs Ctegory Profit Heatmap\n",
    "\n",
    "# Goal: check which combinations of Region & Category work best\n",
    "\n",
    "region_category = (\n",
    "    df.groupby(['region', 'category'])['profit']\n",
    "      .sum()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "pivot_rc = region_category.pivot(index=\"region\", columns=\"category\", values=\"profit\")\n",
    "pivot_rc\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.heatmap(\n",
    "#     pivot_rc,\n",
    "#     annot=True,\n",
    "#     fmt=\".0f\"\n",
    "# )\n",
    "# plt.title(\"Profit by Region and Category\")\n",
    "# plt.xlabel(\"Category\")\n",
    "# plt.ylabel(\"Region\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# West is the strongest region overall, driven by very high profits in Office Supplies and strong performance in Technology.\n",
    "# East also performs well across all categories, with especially strong profit in Technology and Office Supplies.\n",
    "# South shows moderate profitability in every category, with balanced results but lower totals than West and East.\n",
    "# Central underperforms: Furniture is unprofitable, and although Technology and Office Supplies generate positive profit, they are much lower than in other regions.\n",
    "\n",
    "# 6.5 Prepare Data for Mapping (for PowerBI)\n",
    "# Goadl: we won't draw the actual geographical map in notebook, but will prepare a clean dataset\n",
    "\n",
    "geo_export = states_perf.copy()\n",
    "geo_export.to_csv(\"../output/state_performance.csv\", index=False)\n",
    "geo_export.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d1023",
   "metadata": {},
   "source": [
    "### Geographic Analysis\n",
    "\n",
    "- West is the srongest region overall, leading in both sales and profit, with the srongest profit margins in all product categories\n",
    "- East is close second one, consistently profitable with solid margins, especially in Technology and Office Suppliers\n",
    "- South performs moderately well, generating positive profit in all produt categories, bu less than for West and East\n",
    "- Central underperforms significantly, the only one showing overall negative profit margin, driven largerly by losses in Furniture Category\n",
    "- At State level, California and New Yourk are the biggest contributers, while Texas, Ohio, Pennsylvania are the biggest loss-making states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a02620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Shipping & Operational Metrics\n",
    "\n",
    "# Shipping time is an important operational KPI.\n",
    "# Long delays can reduce customer satisfaction, increase return rates, and impact profit indirectly (especially for large or fragile items).\n",
    "\n",
    "# Messure average shipping delay\n",
    "# Compare average delay by ship mode\n",
    "# See whether delay affects profit or correlates with discount\n",
    "# Identify operational inefficiencies across the dataset\n",
    "\n",
    "# 7.1 Explore Shipping Delay Distribution\n",
    "# Goal: understand how many days it usually takes to deliver orders.\n",
    "\n",
    "df['ship_delay'] = (df['ship_date'] - df['order_date']).dt.days\n",
    "df['ship_delay'].describe()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.histplot(df['shipping_delay'], bins=20, kde=True)\n",
    "# plt.title(\"Distribution of Shipping Delay (Days)\")\n",
    "# plt.xlabel(\"Days\")\n",
    "# plt.ylabel(\"Count of Orders\")\n",
    "# plt.show()\n",
    "\n",
    "# Most orders are delivered in 2–6 days, with 4 days being the most common delivery time.\n",
    "# A smaller number of orders arrive same-day (0 days) or after 7 days, indicating occasional extremes.\n",
    "# The distribution shows a slight right-skew, meaning delays of 5–7 days occur more often than very fast deliveries.\n",
    "# Overall, shipping performance is consistent, but long-delay cases (6–7 days) may need operational review.\n",
    "\n",
    "# 7.2 Compare ship delays by Shipping Mode\n",
    "\n",
    "shipmode_delay = (\n",
    "    df.groupby('ship_mode')['shipping_delay']\n",
    "      .mean()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "shipmode_delay\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.barplot(\n",
    "#     x=shipmode_delay.values,\n",
    "#     y=shipmode_delay.index\n",
    "# )\n",
    "# plt.title(\"Average Shipping Delay by Ship Mode\")\n",
    "# plt.xlabel(\"Avg Delay (Days)\")\n",
    "# plt.ylabel(\"Ship Mode\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation\n",
    "# Same Day shipping performs as expected with near-zero delay,\n",
    "# First Class has the lowest delay among regular shipping options (~2.2 days), showing strong reliability.\n",
    "# Second Class averages a moderate delay (~3.2 days), aligned with its slower service level.\n",
    "# Standard Class has the longest delay (~5 days), which is expected but still significantly longer than other modes.\n",
    "\n",
    "\n",
    "# 7.3. Shipping delay by Category\n",
    "\n",
    "shipmode_delay = (\n",
    "    df.groupby('category')['shipping_delay']\n",
    "      .mean()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "shipmode_delay\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.barplot(\n",
    "#     x=shipmode_delay.values,\n",
    "#     y=shipmode_delay.index\n",
    "# )\n",
    "# plt.title(\"Average Shipping Delay by Category\")\n",
    "# plt.xlabel(\"Avg Shipping Delay (Days)\")\n",
    "# plt.ylabel(\"Category\")\n",
    "# plt.show()\n",
    "\n",
    "# Iterpretation\n",
    "# All three categories (Furniture, Technology, Office Supplies) have very similar average shipping delays, around 4 days.\n",
    "# This suggests delays are driven more by shipping processes than by product type.\n",
    "\n",
    "\n",
    "# 7.4 Correlation between Shipping Delay & Profit\n",
    "# Goal: see whether shipping delay impacts profitability.\n",
    "\n",
    "df[['shipping_delay', 'profit', 'discount']].corr()\n",
    "\n",
    "# Interpretation:\n",
    "# Shipping delay has no meaningful correlation with profit (−0.004). Delivery speed does not affect profitability in this dataset.\n",
    "# Discount has a moderate negative correlation with profit (−0.22), confirming that higher discounts reduce profit.\n",
    "# Shipping delay and discount are uncorrelated (~0.0004), meaning discounting does not influence how fast orders are delivered.\n",
    "\n",
    "# 7.5 Shipping delay vs Discount\n",
    "\n",
    "# Goal: investigate if heavily discounted orders have shorted or longer shipping delays\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# sns.scatterplot(\n",
    "#     data=df,\n",
    "#     x='shipping_delay',\n",
    "#     y='discount',\n",
    "#     alpha=0.3\n",
    "# )\n",
    "# plt.title(\"Shipping Delay vs Discount\")\n",
    "# plt.xlabel(\"Shipping Delay (Days)\")\n",
    "# plt.ylabel(\"Discount\")\n",
    "# plt.show()\n",
    "\n",
    "# Interpretation:\n",
    "# Order discounts evenly distributed across different shipping days, that means no clear relationship between shipping delay and discount\n",
    "\n",
    "# 7.6. Flag Problematic Orders (Long Delay + Low Profit)\n",
    "\n",
    "problematic_orders = df[(df['shipping_delay'] > 6) & (df['profit'] < 0)]\n",
    "problematic_orders\n",
    "\n",
    "# 1. Number of problematic orders\n",
    "num_problem = len(problematic_orders) \n",
    "\n",
    "# 2. Total loss\n",
    "total_loss = df['profit'].sum() \n",
    "\n",
    "# 3. Average loss per order\n",
    "avg_loss_per_order = problematic_orders['profit'].mean()\n",
    "\n",
    "# 4. Identify which categories are responsible\n",
    "problematic_orders['category'].value_counts()\n",
    "problematic_orders.groupby('category')['profit'].sum().sort_values()\n",
    "\n",
    "\n",
    "# 5. Check product types\n",
    "problematic_orders.groupby('product_name')['profit'].sum().sort_values().head(10)\n",
    "\n",
    "# 6. Check regions\n",
    "problematic_orders.groupby('region')['profit'].sum().sort_values()\n",
    "\n",
    "\n",
    "# 7. Check ship mode\n",
    "problematic_orders.groupby('ship_mode')['profit'].sum().sort_values()\n",
    "\n",
    "# Interpretation\n",
    "\n",
    "# 115 orders suffered from both long shipping delays (>6 days) and negative profit, creating operational and financial issues.\n",
    "# These orders generated a total loss of –$16,491, with an average loss of –$143 per order.\n",
    "\n",
    "# Which categories are responsible\n",
    "\n",
    "# Most problematic orders come from Office Supplies (64 orders), followed by Furniture (35) and Technology (16).\n",
    "# However, Technology orders caused the largest dollar loss (–$8.3K), indicating that delayed tech items are especially costly\n",
    "\n",
    "# Worst products\n",
    "\n",
    "# Losses are driven by bulky or high-shipping-cost items such as:\n",
    "# Cubify 3D Printers,\n",
    "# Binding machines,\n",
    "# Large conference tables,\n",
    "# Specialized office equipment.\n",
    "\n",
    "# Regional impact\n",
    "\n",
    "# Losses are concentrated in the East (–$9.2K), followed by South, West, and Central,\n",
    "\n",
    "# Shipping mode\n",
    "\n",
    "# 100% of problematic orders came from Standard Class.\n",
    "\n",
    "# Key Insight\n",
    "# Long delays combined with Standard Class shipping and heavy/bulky products—especially Technology—are the primary drivers of avoidable losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665c020",
   "metadata": {},
   "source": [
    "### Shipping Analysis Summary\n",
    "\n",
    "- Most orders arrive within 2–6 days, with 4 days being the peak; delays of 6–7 days are operational outliers.\n",
    "- Shipping speed aligns with expectations: Same Day is fastest, First Class performs well, and Standard Class is the slowest.\n",
    "- All product categories (Furniture, Technology, Office Supplies) show similar average delays (~4 days), meaning delays come from logistics, not product type.\n",
    "- Shipping delay has no meaningful correlation with profit — delivery time does not directly affect profitability.\n",
    "- However, 115 problematic orders had long delays + negative profit, causing $16.5K in losses.\n",
    "- These issues are driven by bulky, high-cost items, especially in Technology, and occur mostly in the East region.\n",
    "- Standard Class accounts for all problematic orders\n",
    "\n",
    "Conclusion:\n",
    "Loss-making delayed orders are rare but predictable — they are heavy items shipped via Standard Class. Optimizing shipping strategy for these SKUs would significantly reduce operational losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02990733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd88dc",
   "metadata": {},
   "source": [
    "Key Business Insights\n",
    "1. Overall Business Performance\n",
    "\n",
    "The Superstore generated ≈$2.3M in total sales over the analyzed period.\n",
    "Total profit ≈ $286K, resulting in an overall profit margin of ~12.5%.\n",
    "There were ~5,000 total orders, showing moderate order volume for a retail operation of this scale.\n",
    "\n",
    "2. Category-Level Profitability\n",
    "\n",
    "Technology is the most profitable category, contributing the largest share of total profit.\n",
    "Office Supplies is profitable but significantly less than Technology.\n",
    "Furniture shows very low profitability and contains several loss-making sub-categories.\n",
    "\n",
    "Insight: Business should prioritize Technology while reviewing discounting and pricing strategy for Furniture.\n",
    "\n",
    "3. Sub-Category Performance\n",
    "\n",
    "Phones and Chairs lead in sales volume.\n",
    "Several sub-categories such as Tables, Bookcases, and high-ticket items in Furniture have consistently negative profit margins.\n",
    "\n",
    "Insight: These unprofitable sub-categories may require renegotiation with suppliers, better pricing strategy, or reduced discounting.\n",
    "\n",
    "4. Regional Performance\n",
    "\n",
    "West and East regions show the strongest sales & profit performance.\n",
    "South region performs moderately.\n",
    "Central region contains the highest concentration of negative-profit items, pulling down overall profitability.\n",
    "\n",
    "Insight: Central region requires deeper investigation into pricing, logistics costs, or customer mix.\n",
    "\n",
    "5. Discount Impact\n",
    "\n",
    "Scatterplot analysis shows:\n",
    "Discounts above 20–30% often drive profit into negative territory.\n",
    "Many high-discount transactions have loss-making profit margins.\n",
    "Discounting is a major driver of profit erosion.\n",
    "\n",
    "Insight: Implement stricter discount policies, especially for Furniture & Central region customers.\n",
    "\n",
    "6. Customer Profitability\n",
    "\n",
    "Profit contribution is heavily skewed — a classic Pareto (80/20) pattern.\n",
    "A small group of top customers drives a significant portion of profit.\n",
    "“Top 10 Customers by Profit” consistently deliver strong value.\n",
    "Conversely, “Bottom 10 Products” and some customer segments contribute negative profit, mostly due to discount-heavy orders.\n",
    "\n",
    "Insight: Strengthen relationships with top customers, and evaluate loss-making customer/product patterns.\n",
    "\n",
    "7. Profit Margin Distribution\n",
    "\n",
    "Margin distribution is highly skewed:\n",
    "Most orders fall between 0% and 20% margin.\n",
    "A long left-tail of orders exhibits negative margins, revealing opportunities for margin improvement.\n",
    "\n",
    "Insight: Many products operate at razor-thin margins—pricing strategy requires optimization.\n",
    "\n",
    "8. Product-Level Risk\n",
    "Bottom 10 most unprofitable products show:\n",
    "Losses up to -$8K per product.\n",
    "Majority belong to Furniture and high-discount items.\n",
    "These products represent repeated loss-making behavior.\n",
    "\n",
    "Insight: These products require immediate intervention—either reprice, reduce discounting, or remove from catalog.\n",
    "\n",
    "9. Seasonal Trends\n",
    "\n",
    "Monthly sales trend shows clear seasonality, with:\n",
    "Sales spikes occurring in late-year months.\n",
    "Noticeable mid-year dips.\n",
    "\n",
    "Insight: Demand forecasting and inventory planning should account for seasonal peaks.\n",
    "\n",
    "Summary: Strategic Recommendations\n",
    "\n",
    "Reduce aggressive discounting, especially above 20%.\n",
    "Reassess unprofitable products (Tables, Bookcases, some high-ticket Furniture items).\n",
    "Improve profitability in the Central region through logistics, pricing, or customer segmentation review.\n",
    "Enhance pricing strategy for margin-sensitive categories.\n",
    "Focus marketing efforts on top customers who consistently deliver profit.\n",
    "Optimize the product portfolio, removing or repricing persistent loss-makers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
